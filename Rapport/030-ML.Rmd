
# Modélisation de la probabilité de fusionner

```{r}
load("../Base.RData")
load("../DiagML.RData")
```

## Modèles testés

Après nettoyage et sélection des features, la base de données pour exécuter le machine learning est donc constituée de 109082 observations et de 38 variables explicatives.

```{r}
names(baseML[,-39])
```

On découpe cette base en 12 blocs de lignes qui vont permettre de réaliser la validation croisée. On a également implémenté une fonction qui permet d'entrainer puis de tester les modèles suivants :

- régression logistique
- régression logistique avec choix de variables par AIC (step)
- régressions pénalisées (lasso, ridge, elasticnet)
- arbre de décision
- boosting (adaboost)
- forêt aléatoire
- SVM (mais n'ont pas abouti)
- Réseaux de neurones
    + 3 couches connectées de façon complète avec respectivement 10, 20 et 10 neurones
    + 4 couches connectées de façon complète avec respectivement 1024, 640, 120 et 12 neurones

La fonction est appelée dans une boucle `foreach` qui permet de paraléliser les traitements pour chaque bloc. On a ainsi pu diviser les temps de calcul par 3 (pour les modèles entrainés sur nos postes) ou par 7 (pour les modèles entraînés sur un serveur).
Le résultat de cette boucle est un dataframe contenant les prévisions (probabilités) faites par les différents modèles sur les blocs de tests.

Les résultats sont présentés dans l'application shiny dédiée au projet (courbes ROC, indicateur AUC et matrices de confusion).

## Quelques résultats

Globalement, on constate que les modèles à base d'arbres de décisions donnent de meilleurs résultat : il semble que le choix de fusionner soit déterministe, et guidé les proximités en termes de vote des habitants (vote modéré ou extrême). On peut penser que cette variable détermine également les habitudes collaboratives des communes, c'est à dire leur appartenance à une même structure intercommunale (EPCI, SCOT), et que les communes ayant des électorats proches peuvent plus facilement se regrouper dans le cadre d'un projet commun.



```{r}
par(mfrow=c(1,2))
lapply(prev.roc,plot)
lapply(prev[,-1],function(x) plot(density(x),col="blue"))
lapply(prev,function(x) sum(x>.5))
```

L'arbre de décision seul produisant déjà des résultats très satisfaisants, les forêts aléatoires et adaboost l'améliorent, mais légèrement. Les autres méthodes sont moins performantes. La régression par exemple, ne met autant en avant le pouvoir discriminant de la proximité politique

```{r}
arbre <- rpart(data=baseML,y~.)
visTree(arbre)
glm(data=baseML,formula = y~.,family=binomial) %>% summary()
```

