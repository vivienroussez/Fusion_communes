
# Analyse des features

Nous regardons la distribution d'une variable de chaque grande famille de données

```{r}
load("../Base.RData")
#On met en facteur la variable de fusion
dat1 <- baseML
dat1$fusion<-as.factor(dat1$y)
#on crée un identifiant
dat1$indic<-rownames(dat1)
```


## La variable de distance sur la population: dist_P13_POP

```{r}
library(ggplot2)
ggplot(dat1,aes(x=fusion,y=dist_P13_POP))+geom_violin()+geom_boxplot(width=0.1)
```

On constate qu'il y a beaucoup de couple de communes qui ont une faible différence de population lors de la non-fusion. On retrouve le cas pour les communes rurales ou les grandes villes.  

## La variable sur l'appartenance politique: dist_Pol1

```{r}
ggplot(dat1,aes(x=fusion,y=dist_Pol1)) + geom_violin() + geom_boxplot(width=0.1) +
  xlab("Fusion") + ylab("Part de couples de communes")
```

On constate, sur les non-fusions, une grande disparité des valeurs, ce qui n'est pas le cas pour la fusion. Ce qui confirmerait que les communes ayant fusionné ont une même tendance politique.  

## La variable du nombre de navettes inter-communes : nb_navettes  

```{r}
ggplot(dat1,aes(x=fusion,y=log(nb_navettes)))+geom_violin()+geom_boxplot(width=0.1)+
  xlab("Fusion") + ylab("Part de couples de communes")
```

Nous sommes passés au logarithme afin de mieux visualiser la distribution, très asymétrique.

## Matrice des corrélations

```{r}
#install.packages("corrplot")
library(corrplot)
library(dplyr)
mat_cor<-select(dat1,starts_with("nb"),starts_with("dist"))
dat1_cor<-round(cor(mat_cor),2)

#corrplot(dat1_cor, method = "circle")
#corrplot(dat1_cor, method = "ellipse")
#corrplot(dat1_cor, method = "number")
corrplot(dat1_cor, method = "pie")
```

On constate qu'il n'y a pas de corrélations linéaires entre les variables de distances et les variables de nb.
On voit qu'il y a des corrélations entre certaines variables. Par exemple, dist_P13_POP et dist_P13_LOC.  

On garde toutes les variables afin de ne pas supprimer de la variance.  

Avant de lancer les méthodes de ML, nous allons faire du clustering afin de voir s'il n'existerait pas de clusters de couples de communes.

## Clustering

Etant donné le nombre important de lignes et la faible capacité de la machine. Nous ne ferons pas de CAH puis un K-means mais un k-means directement. Nous aurions ?galement pu faire un k-means pour diminuer le nombre de lignes et ensuite effectuer une CAH pour maximiser l'inertie interclasse (alors que le k-means minimise l'inertie intraclasse)

```{r}
library(dplyr)
set.seed(1234)
lst_km <- list()
lst_km.ss <- list()
for (ii in 1:20)
{
  lst_km[[ii]] <- kmeans(mat_cor,ii,iter.max = 20)
  lst_km.ss[[ii]] <- lst_km[[ii]]$betweenss/lst_km[[ii]]$totss
}
unlist(lst_km.ss) %>% plot(col="blue",xlab="Nombre de classes",ylab="Variance interclasse")
```

On voit que la variance intraclasse stagne à partir de 6 classes.  
On voit qu'avec les 6 classes, on retrace près de 90% de la variance totale

On affecte, à chaque couple de communes, son cluster d'appartenance et on regarde dans chaque cluster la répartition de la variable fusion 

```{r}
cl <- lst_km[[6]]$cluster

cl <- data.frame(cl)
cl$ident<-rownames(cl)
dat1$ident<-rownames(dat1)

dat1Cl <- merge(dat1,cl,by.x="ident",by.y="ident",all.x=T)

dat1Cl$fusion_class <- ifelse(dat1Cl$fusion=="0","Non Fusion","Fusion")
dat1Cl$cl <- as.factor(dat1Cl$cl)
dat1Cl$cl_class<-paste("cluster",dat1Cl$cl)

tableau<-table(dat1Cl$cl_class,dat1Cl$fusion_class) %>% addmargins()
tableau
```

On constate que le cluster 1 contient la quasi-totalité des fusions. De plus, cette classe est marquée est consituée par les couples de communes les plus semblables :

```{r}
resum <- select(dat1Cl,-cl) %>%
  group_by(cl_class) %>%
  summarise_if(is.numeric,mean) %>% as.data.frame()
kable(resum)
```



